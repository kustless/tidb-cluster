{
  "v1": {
    "config": null,
    "helmValues": "# Default values for tidb-cluster.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\n# Also see monitor.serviceAccount\n# If you set rbac.create to false, you need to provide a value for monitor.serviceAccount\nrbac:\n  create: true\n\n# clusterName is the TiDB cluster name, if not specified, the chart release name will be used\n# clusterName: demo\n\n# Add additional TidbCluster labels\n# ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\nextraLabels: {}\n\n# schedulerName must be same with charts/tidb-operator/values#scheduler.schedulerName\nschedulerName: tidb-scheduler\n\n# timezone is the default system timzone for TiDB\ntimezone: UTC\n\n# default reclaim policy of a PV\npvReclaimPolicy: Retain\n\n# services is the service list to expose, default is ClusterIP\n# can be ClusterIP | NodePort | LoadBalancer\nservices:\n  - name: pd\n    type: ClusterIP\n\ndiscovery:\n  image: pingcap/tidb-operator:v1.0.0-beta.2\n  imagePullPolicy: IfNotPresent\n  resources:\n    limits:\n      cpu: 250m\n      memory: 150Mi\n    requests:\n      cpu: 80m\n      memory: 50Mi\n\n# Whether enable ConfigMap Rollout management.\n# When enabling, change of ConfigMap will trigger a graceful rolling-update of the component.\n# This feature is only available in tidb-operator v1.0 or higher.\n# Note: Switch this variable against an existing cluster will cause an rolling-update of each component even\n# if the ConfigMap was not changed.\nenableConfigMapRollout: false\n\npd:\n  replicas: 3\n  image: pingcap/pd:v3.0.0-rc.1\n  logLevel: info\n  # storageClassName is a StorageClass provides a way for administrators to describe the \"classes\" of storage they offer.\n  # different classes might map to quality-of-service levels, or to backup policies,\n  # or to arbitrary policies determined by the cluster administrators.\n  # refer to https://kubernetes.io/docs/concepts/storage/storage-classes\n  storageClassName: local-storage\n\n  # Image pull policy.\n  imagePullPolicy: IfNotPresent\n\n  # maxStoreDownTime is how long a store will be considered `down` when disconnected\n  # if a store is considered `down`, the regions will be migrated to other stores\n  maxStoreDownTime: 30m\n  # maxReplicas is the number of replicas for each region\n  maxReplicas: 3\n  resources:\n    limits: {}\n    #   cpu: 8000m\n    #   memory: 8Gi\n    requests:\n      # cpu: 4000m\n      # memory: 4Gi\n      storage: 1Gi\n\n  ## affinity defines pd scheduling rules,it's default settings is empty.\n  ## please read the affinity document before set your scheduling rule:\n  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n  affinity: {}\n  ## The following is typical example of affinity settings:\n  ## The PodAntiAffinity setting of the example keeps PD pods does not co-locate on a topology node as far as possible to improve the disaster tolerance of PD on Kubernetes.\n  ## The NodeAffinity setting of the example ensure that the PD pods can only be scheduled to nodes with label:[type=\"pd\"],\n  # affinity:\n  #   podAntiAffinity:\n  #     preferredDuringSchedulingIgnoredDuringExecution:\n  #     # this term work when the nodes have the label named region\n  #     - weight: 10\n  #       podAffinityTerm:\n  #         labelSelector:\n  #           matchLabels:\n  #             app.kubernetes.io/instance: \u003crelease name\u003e\n  #             app.kubernetes.io/component: \"pd\"\n  #         topologyKey: \"region\"\n  #         namespaces:\n  #         - \u003chelm namespace\u003e\n  #     # this term work when the nodes have the label named zone\n  #     - weight: 20\n  #       podAffinityTerm:\n  #         labelSelector:\n  #           matchLabels:\n  #             app.kubernetes.io/instance: \u003crelease name\u003e\n  #             app.kubernetes.io/component: \"pd\"\n  #         topologyKey: \"zone\"\n  #         namespaces:\n  #         - \u003chelm namespace\u003e\n  #     # this term work when the nodes have the label named rack\n  #     - weight: 40\n  #       podAffinityTerm:\n  #         labelSelector:\n  #           matchLabels:\n  #             app.kubernetes.io/instance: \u003crelease name\u003e\n  #             app.kubernetes.io/component: \"pd\"\n  #         topologyKey: \"rack\"\n  #         namespaces:\n  #         - \u003chelm namespace\u003e\n  #     # this term work when the nodes have the label named kubernetes.io/hostname\n  #     - weight: 80\n  #       podAffinityTerm:\n  #         labelSelector:\n  #           matchLabels:\n  #             app.kubernetes.io/instance: \u003crelease name\u003e\n  #             app.kubernetes.io/component: \"pd\"\n  #         topologyKey: \"kubernetes.io/hostname\"\n  #         namespaces:\n  #         - \u003chelm namespace\u003e\n  #   nodeAffinity:\n  #     requiredDuringSchedulingIgnoredDuringExecution:\n  #       nodeSelectorTerms:\n  #       - matchExpressions:\n  #         - key: \"kind\"\n  #           operator: In\n  #           values:\n  #           - \"pd\"\n\n  ## nodeSelector ensure pods only assigning to nodes which have each of the indicated key-value pairs as labels\n  ## ref:https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector\n  nodeSelector: {}\n\n  ## Tolerations are applied to pods, and allow pods to schedule onto nodes with matching taints.\n  ## refer to https://kubernetes.io/docs/concepts/configuration/taint-and-toleration\n  tolerations: []\n  # - key: node-role\n  #   operator: Equal\n  #   value: tidb\n  #   effect: \"NoSchedule\"\n  annotations: {}\n\ntikv:\n  replicas: 3\n  image: pingcap/tikv:v3.0.0-rc.1\n  logLevel: info\n  # storageClassName is a StorageClass provides a way for administrators to describe the \"classes\" of storage they offer.\n  # different classes might map to quality-of-service levels, or to backup policies,\n  # or to arbitrary policies determined by the cluster administrators.\n  # refer to https://kubernetes.io/docs/concepts/storage/storage-classes\n  storageClassName: local-storage\n\n  # Image pull policy.\n  imagePullPolicy: IfNotPresent\n\n  # syncLog is a bool value to enable or disable syc-log for raftstore, default is true\n  # enable this can prevent data loss when power failure\n  syncLog: true\n  # size of thread pool for grpc server.\n  # grpcConcurrency: 4\n  resources:\n    limits: {}\n    #   cpu: 16000m\n    #   memory: 32Gi\n    #   storage: 300Gi\n    requests:\n      # cpu: 12000m\n      # memory: 24Gi\n      storage: 10Gi\n\n  ## affinity defines tikv scheduling rules,affinity default settings is empty.\n  ## please read the affinity document before set your scheduling rule:\n  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n  affinity: {}\n\n  ## nodeSelector ensure pods only assigning to nodes which have each of the indicated key-value pairs as labels\n  ## ref:https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector\n  nodeSelector: {}\n\n  ## Tolerations are applied to pods, and allow pods to schedule onto nodes with matching taints.\n  ## refer to https://kubernetes.io/docs/concepts/configuration/taint-and-toleration\n  tolerations: []\n  # - key: node-role\n  #   operator: Equal\n  #   value: tidb\n  #   effect: \"NoSchedule\"\n  annotations: {}\n\n  # block-cache used to cache uncompressed blocks, big block-cache can speed up read.\n  # in normal cases should tune to 30%-50% tikv.resources.limits.memory\n  # defaultcfBlockCacheSize: \"1GB\"\n\n  # in normal cases should tune to 10%-30% tikv.resources.limits.memory\n  # writecfBlockCacheSize: \"256MB\"\n\n  # size of thread pool for high-priority/normal-priority/low-priority operations\n  # readpoolStorageConcurrency: 4\n\n  # Notice: if tikv.resources.limits.cpu \u003e 8, default thread pool size for coprocessors\n  # will be set to tikv.resources.limits.cpu * 0.8.\n  # readpoolCoprocessorConcurrency: 8\n\n  # scheduler's worker pool size, should increase it in heavy write cases,\n  # also should less than total cpu cores.\n  # storageSchedulerWorkerPoolSize: 4\n\ntidb:\n  replicas: 2\n  # The secret name of root password, you can create secret with following command:\n  # kubectl create secret generic tidb-secret --from-literal=root=\u003croot-password\u003e --namespace=\u003cnamespace\u003e\n  # If unset, the root password will be empty and you can set it after connecting\n  # passwordSecretName: tidb-secret\n  # initSql is the SQL statements executed after the TiDB cluster is bootstrapped.\n  # initSql: |-\n  #   create database app;\n  image: pingcap/tidb:v3.0.0-rc.1\n  # Image pull policy.\n  imagePullPolicy: IfNotPresent\n  logLevel: info\n  preparedPlanCacheEnabled: false\n  preparedPlanCacheCapacity: 100\n  # Enable local latches for transactions. Enable it when\n  # there are lots of conflicts between transactions.\n  txnLocalLatchesEnabled: false\n  txnLocalLatchesCapacity: \"10240000\"\n  # The limit of concurrent executed sessions.\n  tokenLimit: \"1000\"\n  # Set the memory quota for a query in bytes. Default: 32GB\n  memQuotaQuery: \"34359738368\"\n  # The limitation of the number for the entries in one transaction.\n  # If using TiKV as the storage, the entry represents a key/value pair.\n  # WARNING: Do not set the value too large, otherwise it will make a very large impact on the TiKV cluster.\n  # Please adjust this configuration carefully.\n  txnEntryCountLimit: \"300000\"\n  # The limitation of the size in byte for the entries in one transaction.\n  # If using TiKV as the storage, the entry represents a key/value pair.\n  # WARNING: Do not set the value too large, otherwise it will make a very large impact on the TiKV cluster.\n  # Please adjust this configuration carefully.\n  txnTotalSizeLimit: \"104857600\"\n  # enableBatchDml enables batch commit for the DMLs\n  enableBatchDml: false\n  # check mb4 value in utf8 is used to control whether to check the mb4 characters when the charset is utf8.\n  checkMb4ValueInUtf8: true\n  # treat-old-version-utf8-as-utf8mb4 use for upgrade compatibility. Set to true will treat old version table/column UTF8 charset as UTF8MB4.\n  treatOldVersionUtf8AsUtf8mb4: true\n  # lease is schema lease duration, very dangerous to change only if you know what you do.\n  lease: 45s\n  # Max CPUs to use, 0 use number of CPUs in the machine.\n  maxProcs: 0\n  resources:\n    limits: {}\n    #   cpu: 16000m\n    #   memory: 16Gi\n    requests: {}\n    #   cpu: 12000m\n    #   memory: 12Gi\n\n\n  ## affinity defines tikv scheduling rules,affinity default settings is empty.\n  ## please read the affinity document before set your scheduling rule:\n  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n  affinity: {}\n\n  ## nodeSelector ensure pods only assigning to nodes which have each of the indicated key-value pairs as labels\n  ## ref:https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector\n  nodeSelector: {}\n\n  ## Tolerations are applied to pods, and allow pods to schedule onto nodes with matching taints.\n  ## refer to https://kubernetes.io/docs/concepts/configuration/taint-and-toleration\n  tolerations: []\n  # - key: node-role\n  #   operator: Equal\n  #   value: tidb\n  #   effect: \"NoSchedule\"\n  annotations: {}\n  maxFailoverCount: 3\n  service:\n    type: NodePort\n    exposeStatus: true\n    # annotations:\n      # cloud.google.com/load-balancer-type: Internal\n  # separateSlowLog: true\n  slowLogTailer:\n    image: busybox:1.26.2\n    resources:\n      limits:\n        cpu: 100m\n        memory: 50Mi\n      requests:\n        cpu: 20m\n        memory: 5Mi\n\n  # tidb plugin configuration\n  plugin:\n    # enable plugin or not\n    enable: false\n    # the start argument to specify the folder containing\n    directory: /plugins\n    # the start argument to specify the plugin id (name \"-\" version) that needs to be loaded, e.g. 'conn_limit-1'.\n    list: [\"whitelist-1\"]\n\n# mysqlClient is used to set password for TiDB\n# it must has Python MySQL client installed\nmysqlClient:\n  image: tnir/mysqlclient\n  imagePullPolicy: IfNotPresent\n\nmonitor:\n  create: true\n  # Also see rbac.create\n  # If you set rbac.create to false, you need to provide a value here.\n  # If you set rbac.create to true, you should leave this empty.\n  # serviceAccount:\n  persistent: false\n  storageClassName: local-storage\n  storage: 10Gi\n  grafana:\n    create: true\n    image: grafana/grafana:6.0.1\n    imagePullPolicy: IfNotPresent\n    logLevel: info\n    resources:\n      limits: {}\n      #   cpu: 8000m\n      #   memory: 8Gi\n      requests: {}\n      #   cpu: 4000m\n      #   memory: 4Gi\n    username: admin\n    password: admin\n    config:\n      # Configure Grafana using environment variables except GF_PATHS_DATA, GF_SECURITY_ADMIN_USER and GF_SECURITY_ADMIN_PASSWORD\n      # Ref https://grafana.com/docs/installation/configuration/#using-environment-variables\n      GF_AUTH_ANONYMOUS_ENABLED: \"true\"\n      GF_AUTH_ANONYMOUS_ORG_NAME: \"Main Org.\"\n      GF_AUTH_ANONYMOUS_ORG_ROLE: \"Viewer\"\n      # if grafana is running behind a reverse proxy with subpath http://foo.bar/grafana\n      # GF_SERVER_DOMAIN: foo.bar\n      # GF_SERVER_ROOT_URL: \"%(protocol)s://%(domain)s/grafana/\"\n    service:\n      type: NodePort\n  prometheus:\n    image: prom/prometheus:v2.2.1\n    imagePullPolicy: IfNotPresent\n    logLevel: info\n    resources:\n      limits: {}\n      #   cpu: 8000m\n      #   memory: 8Gi\n      requests: {}\n      #   cpu: 4000m\n      #   memory: 4Gi\n    service:\n      type: NodePort\n    reserveDays: 12\n    # alertmanagerURL: \"\"\n  nodeSelector: {}\n    # kind: monitor\n    # zone: cn-bj1-01,cn-bj1-02\n    # region: cn-bj1\n  tolerations: []\n  # - key: node-role\n  #   operator: Equal\n  #   value: tidb\n  #   effect: \"NoSchedule\"\n\nbinlog:\n  pump:\n    create: false\n    replicas: 1\n    image: pingcap/tidb-binlog:v3.0.0-rc.1\n    imagePullPolicy: IfNotPresent\n    logLevel: info\n    # storageClassName is a StorageClass provides a way for administrators to describe the \"classes\" of storage they offer.\n    # different classes might map to quality-of-service levels, or to backup policies,\n    # or to arbitrary policies determined by the cluster administrators.\n    # refer to https://kubernetes.io/docs/concepts/storage/storage-classes\n    storageClassName: local-storage\n    storage: 10Gi\n    syncLog: true\n    # a integer value to control expiry date of the binlog data, indicates for how long (in days) the binlog data would be stored.\n    # must bigger than 0\n    gc: 7\n    # number of seconds between heartbeat ticks (in 2 seconds)\n    heartbeatInterval: 2\n\n  drainer:\n    create: false\n    image: pingcap/tidb-binlog:v3.0.0-rc.1\n    imagePullPolicy: IfNotPresent\n    logLevel: info\n    # storageClassName is a StorageClass provides a way for administrators to describe the \"classes\" of storage they offer.\n    # different classes might map to quality-of-service levels, or to backup policies,\n    # or to arbitrary policies determined by the cluster administrators.\n    # refer to https://kubernetes.io/docs/concepts/storage/storage-classes\n    storageClassName: local-storage\n    storage: 10Gi\n    # the number of the concurrency of the downstream for synchronization. The bigger the value,\n    # the better throughput performance of the concurrency (16 by default)\n    workerCount: 16\n    # the interval time (in seconds) of detect pumps' status (default 10)\n    detectInterval: 10\n    # disbale detect causality\n    disableDetect: false\n    # disable dispatching sqls that in one same binlog; if set true, work-count and txn-batch would be useless\n    disableDispatch: false\n    # # disable sync these schema\n    ignoreSchemas: \"INFORMATION_SCHEMA,PERFORMANCE_SCHEMA,mysql,test\"\n    # if drainer donesn't have checkpoint, use initial commitTS to initial checkpoint\n    initialCommitTs: 0\n    # enable safe mode to make syncer reentrant\n    safeMode: false\n    # the number of SQL statements of a transaction that are output to the downstream database (20 by default)\n    txnBatch: 20\n    # downstream storage, equal to --dest-db-type\n    # valid values are \"mysql\", \"pb\", \"kafka\"\n    destDBType: pb\n    mysql: {}\n      # host: \"127.0.0.1\"\n      # user: \"root\"\n      # password: \"\"\n      # port: 3306\n      # # Time and size limits for flash batch write\n      # timeLimit: \"30s\"\n      # sizeLimit: \"100000\"\n    kafka: {}\n      # only need config one of zookeeper-addrs and kafka-addrs, will get kafka address if zookeeper-addrs is configed.\n      # zookeeperAddrs: \"127.0.0.1:2181\"\n      # kafkaAddrs: \"127.0.0.1:9092\"\n      # kafkaVersion: \"0.8.2.0\"\n\nscheduledBackup:\n  create: false\n  binlogImage: pingcap/tidb-binlog:v3.0.0-rc.1\n  binlogImagePullPolicy: IfNotPresent\n  # https://github.com/tennix/tidb-cloud-backup\n  mydumperImage: pingcap/tidb-cloud-backup:latest\n  mydumperImagePullPolicy: IfNotPresent\n  # storageClassName is a StorageClass provides a way for administrators to describe the \"classes\" of storage they offer.\n  # different classes might map to quality-of-service levels, or to backup policies,\n  # or to arbitrary policies determined by the cluster administrators.\n  # refer to https://kubernetes.io/docs/concepts/storage/storage-classes\n  storageClassName: local-storage\n  storage: 100Gi\n  # https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#schedule\n  schedule: \"0 0 * * *\"\n  # https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#suspend\n  suspend: false\n  # https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#jobs-history-limits\n  successfulJobsHistoryLimit: 3\n  failedJobsHistoryLimit: 1\n  # https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#starting-deadline\n  startingDeadlineSeconds: 3600\n  # https://github.com/maxbube/mydumper/blob/master/docs/mydumper_usage.rst#options\n  options: \"--chunk-filesize=100\"\n  # secretName is the name of the secret which stores user and password used for backup\n  # Note: you must give the user enough privilege to do the backup\n  # you can create the secret by:\n  # kubectl create secret generic backup-secret --from-literal=user=root --from-literal=password=\u003cpassword\u003e\n  secretName: backup-secret\n  # backup to gcp\n  gcp: {}\n  # bucket: \"\"\n  # secretName is the name of the secret which stores the gcp service account credentials json file\n  # The service account must have read/write permission to the above bucket.\n  # Read the following document to create the service account and download the credentials file as credentials.json:\n  # https://cloud.google.com/docs/authentication/production#obtaining_and_providing_service_account_credentials_manually\n  # And then create the secret by: kubectl create secret generic gcp-backup-secret --from-file=./credentials.json\n  # secretName: gcp-backup-secret\n\n  # backup to ceph object storage\n  ceph: {}\n  # endpoint: \"\"\n  # bucket: \"\"\n  # secretName is the name of the secret which stores ceph object store access key and secret key\n  # You can create the secret by:\n  # kubectl create secret generic ceph-backup-secret --from-literal=access_key=\u003caccess-key\u003e --from-literal=secret_key=\u003csecret-key\u003e\n  # secretName: ceph-backup-secret\n\nmetaInstance: \"{{ $labels.instance }}\"\nmetaType: \"{{ $labels.type }}\"\nmetaValue: \"{{ $value }}\"\n",
    "releaseName": "tidb-cluster",
    "helmValuesDefaults": "# Default values for tidb-cluster.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\n# Also see monitor.serviceAccount\n# If you set rbac.create to false, you need to provide a value for monitor.serviceAccount\nrbac:\n  create: true\n\n# clusterName is the TiDB cluster name, if not specified, the chart release name will be used\n# clusterName: demo\n\n# Add additional TidbCluster labels\n# ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\nextraLabels: {}\n\n# schedulerName must be same with charts/tidb-operator/values#scheduler.schedulerName\nschedulerName: tidb-scheduler\n\n# timezone is the default system timzone for TiDB\ntimezone: UTC\n\n# default reclaim policy of a PV\npvReclaimPolicy: Retain\n\n# services is the service list to expose, default is ClusterIP\n# can be ClusterIP | NodePort | LoadBalancer\nservices:\n  - name: pd\n    type: ClusterIP\n\ndiscovery:\n  image: pingcap/tidb-operator:v1.0.0-beta.2\n  imagePullPolicy: IfNotPresent\n  resources:\n    limits:\n      cpu: 250m\n      memory: 150Mi\n    requests:\n      cpu: 80m\n      memory: 50Mi\n\n# Whether enable ConfigMap Rollout management.\n# When enabling, change of ConfigMap will trigger a graceful rolling-update of the component.\n# This feature is only available in tidb-operator v1.0 or higher.\n# Note: Switch this variable against an existing cluster will cause an rolling-update of each component even\n# if the ConfigMap was not changed.\nenableConfigMapRollout: false\n\npd:\n  replicas: 3\n  image: pingcap/pd:v3.0.0-rc.1\n  logLevel: info\n  # storageClassName is a StorageClass provides a way for administrators to describe the \"classes\" of storage they offer.\n  # different classes might map to quality-of-service levels, or to backup policies,\n  # or to arbitrary policies determined by the cluster administrators.\n  # refer to https://kubernetes.io/docs/concepts/storage/storage-classes\n  storageClassName: local-storage\n\n  # Image pull policy.\n  imagePullPolicy: IfNotPresent\n\n  # maxStoreDownTime is how long a store will be considered `down` when disconnected\n  # if a store is considered `down`, the regions will be migrated to other stores\n  maxStoreDownTime: 30m\n  # maxReplicas is the number of replicas for each region\n  maxReplicas: 3\n  resources:\n    limits: {}\n    #   cpu: 8000m\n    #   memory: 8Gi\n    requests:\n      # cpu: 4000m\n      # memory: 4Gi\n      storage: 1Gi\n\n  ## affinity defines pd scheduling rules,it's default settings is empty.\n  ## please read the affinity document before set your scheduling rule:\n  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n  affinity: {}\n  ## The following is typical example of affinity settings:\n  ## The PodAntiAffinity setting of the example keeps PD pods does not co-locate on a topology node as far as possible to improve the disaster tolerance of PD on Kubernetes.\n  ## The NodeAffinity setting of the example ensure that the PD pods can only be scheduled to nodes with label:[type=\"pd\"],\n  # affinity:\n  #   podAntiAffinity:\n  #     preferredDuringSchedulingIgnoredDuringExecution:\n  #     # this term work when the nodes have the label named region\n  #     - weight: 10\n  #       podAffinityTerm:\n  #         labelSelector:\n  #           matchLabels:\n  #             app.kubernetes.io/instance: \u003crelease name\u003e\n  #             app.kubernetes.io/component: \"pd\"\n  #         topologyKey: \"region\"\n  #         namespaces:\n  #         - \u003chelm namespace\u003e\n  #     # this term work when the nodes have the label named zone\n  #     - weight: 20\n  #       podAffinityTerm:\n  #         labelSelector:\n  #           matchLabels:\n  #             app.kubernetes.io/instance: \u003crelease name\u003e\n  #             app.kubernetes.io/component: \"pd\"\n  #         topologyKey: \"zone\"\n  #         namespaces:\n  #         - \u003chelm namespace\u003e\n  #     # this term work when the nodes have the label named rack\n  #     - weight: 40\n  #       podAffinityTerm:\n  #         labelSelector:\n  #           matchLabels:\n  #             app.kubernetes.io/instance: \u003crelease name\u003e\n  #             app.kubernetes.io/component: \"pd\"\n  #         topologyKey: \"rack\"\n  #         namespaces:\n  #         - \u003chelm namespace\u003e\n  #     # this term work when the nodes have the label named kubernetes.io/hostname\n  #     - weight: 80\n  #       podAffinityTerm:\n  #         labelSelector:\n  #           matchLabels:\n  #             app.kubernetes.io/instance: \u003crelease name\u003e\n  #             app.kubernetes.io/component: \"pd\"\n  #         topologyKey: \"kubernetes.io/hostname\"\n  #         namespaces:\n  #         - \u003chelm namespace\u003e\n  #   nodeAffinity:\n  #     requiredDuringSchedulingIgnoredDuringExecution:\n  #       nodeSelectorTerms:\n  #       - matchExpressions:\n  #         - key: \"kind\"\n  #           operator: In\n  #           values:\n  #           - \"pd\"\n\n  ## nodeSelector ensure pods only assigning to nodes which have each of the indicated key-value pairs as labels\n  ## ref:https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector\n  nodeSelector: {}\n\n  ## Tolerations are applied to pods, and allow pods to schedule onto nodes with matching taints.\n  ## refer to https://kubernetes.io/docs/concepts/configuration/taint-and-toleration\n  tolerations: []\n  # - key: node-role\n  #   operator: Equal\n  #   value: tidb\n  #   effect: \"NoSchedule\"\n  annotations: {}\n\ntikv:\n  replicas: 3\n  image: pingcap/tikv:v3.0.0-rc.1\n  logLevel: info\n  # storageClassName is a StorageClass provides a way for administrators to describe the \"classes\" of storage they offer.\n  # different classes might map to quality-of-service levels, or to backup policies,\n  # or to arbitrary policies determined by the cluster administrators.\n  # refer to https://kubernetes.io/docs/concepts/storage/storage-classes\n  storageClassName: local-storage\n\n  # Image pull policy.\n  imagePullPolicy: IfNotPresent\n\n  # syncLog is a bool value to enable or disable syc-log for raftstore, default is true\n  # enable this can prevent data loss when power failure\n  syncLog: true\n  # size of thread pool for grpc server.\n  # grpcConcurrency: 4\n  resources:\n    limits: {}\n    #   cpu: 16000m\n    #   memory: 32Gi\n    #   storage: 300Gi\n    requests:\n      # cpu: 12000m\n      # memory: 24Gi\n      storage: 10Gi\n\n  ## affinity defines tikv scheduling rules,affinity default settings is empty.\n  ## please read the affinity document before set your scheduling rule:\n  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n  affinity: {}\n\n  ## nodeSelector ensure pods only assigning to nodes which have each of the indicated key-value pairs as labels\n  ## ref:https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector\n  nodeSelector: {}\n\n  ## Tolerations are applied to pods, and allow pods to schedule onto nodes with matching taints.\n  ## refer to https://kubernetes.io/docs/concepts/configuration/taint-and-toleration\n  tolerations: []\n  # - key: node-role\n  #   operator: Equal\n  #   value: tidb\n  #   effect: \"NoSchedule\"\n  annotations: {}\n\n  # block-cache used to cache uncompressed blocks, big block-cache can speed up read.\n  # in normal cases should tune to 30%-50% tikv.resources.limits.memory\n  # defaultcfBlockCacheSize: \"1GB\"\n\n  # in normal cases should tune to 10%-30% tikv.resources.limits.memory\n  # writecfBlockCacheSize: \"256MB\"\n\n  # size of thread pool for high-priority/normal-priority/low-priority operations\n  # readpoolStorageConcurrency: 4\n\n  # Notice: if tikv.resources.limits.cpu \u003e 8, default thread pool size for coprocessors\n  # will be set to tikv.resources.limits.cpu * 0.8.\n  # readpoolCoprocessorConcurrency: 8\n\n  # scheduler's worker pool size, should increase it in heavy write cases,\n  # also should less than total cpu cores.\n  # storageSchedulerWorkerPoolSize: 4\n\ntidb:\n  replicas: 2\n  # The secret name of root password, you can create secret with following command:\n  # kubectl create secret generic tidb-secret --from-literal=root=\u003croot-password\u003e --namespace=\u003cnamespace\u003e\n  # If unset, the root password will be empty and you can set it after connecting\n  # passwordSecretName: tidb-secret\n  # initSql is the SQL statements executed after the TiDB cluster is bootstrapped.\n  # initSql: |-\n  #   create database app;\n  image: pingcap/tidb:v3.0.0-rc.1\n  # Image pull policy.\n  imagePullPolicy: IfNotPresent\n  logLevel: info\n  preparedPlanCacheEnabled: false\n  preparedPlanCacheCapacity: 100\n  # Enable local latches for transactions. Enable it when\n  # there are lots of conflicts between transactions.\n  txnLocalLatchesEnabled: false\n  txnLocalLatchesCapacity: \"10240000\"\n  # The limit of concurrent executed sessions.\n  tokenLimit: \"1000\"\n  # Set the memory quota for a query in bytes. Default: 32GB\n  memQuotaQuery: \"34359738368\"\n  # The limitation of the number for the entries in one transaction.\n  # If using TiKV as the storage, the entry represents a key/value pair.\n  # WARNING: Do not set the value too large, otherwise it will make a very large impact on the TiKV cluster.\n  # Please adjust this configuration carefully.\n  txnEntryCountLimit: \"300000\"\n  # The limitation of the size in byte for the entries in one transaction.\n  # If using TiKV as the storage, the entry represents a key/value pair.\n  # WARNING: Do not set the value too large, otherwise it will make a very large impact on the TiKV cluster.\n  # Please adjust this configuration carefully.\n  txnTotalSizeLimit: \"104857600\"\n  # enableBatchDml enables batch commit for the DMLs\n  enableBatchDml: false\n  # check mb4 value in utf8 is used to control whether to check the mb4 characters when the charset is utf8.\n  checkMb4ValueInUtf8: true\n  # treat-old-version-utf8-as-utf8mb4 use for upgrade compatibility. Set to true will treat old version table/column UTF8 charset as UTF8MB4.\n  treatOldVersionUtf8AsUtf8mb4: true\n  # lease is schema lease duration, very dangerous to change only if you know what you do.\n  lease: 45s\n  # Max CPUs to use, 0 use number of CPUs in the machine.\n  maxProcs: 0\n  resources:\n    limits: {}\n    #   cpu: 16000m\n    #   memory: 16Gi\n    requests: {}\n    #   cpu: 12000m\n    #   memory: 12Gi\n\n\n  ## affinity defines tikv scheduling rules,affinity default settings is empty.\n  ## please read the affinity document before set your scheduling rule:\n  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n  affinity: {}\n\n  ## nodeSelector ensure pods only assigning to nodes which have each of the indicated key-value pairs as labels\n  ## ref:https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector\n  nodeSelector: {}\n\n  ## Tolerations are applied to pods, and allow pods to schedule onto nodes with matching taints.\n  ## refer to https://kubernetes.io/docs/concepts/configuration/taint-and-toleration\n  tolerations: []\n  # - key: node-role\n  #   operator: Equal\n  #   value: tidb\n  #   effect: \"NoSchedule\"\n  annotations: {}\n  maxFailoverCount: 3\n  service:\n    type: NodePort\n    exposeStatus: true\n    # annotations:\n      # cloud.google.com/load-balancer-type: Internal\n  # separateSlowLog: true\n  slowLogTailer:\n    image: busybox:1.26.2\n    resources:\n      limits:\n        cpu: 100m\n        memory: 50Mi\n      requests:\n        cpu: 20m\n        memory: 5Mi\n\n  # tidb plugin configuration\n  plugin:\n    # enable plugin or not\n    enable: false\n    # the start argument to specify the folder containing\n    directory: /plugins\n    # the start argument to specify the plugin id (name \"-\" version) that needs to be loaded, e.g. 'conn_limit-1'.\n    list: [\"whitelist-1\"]\n\n# mysqlClient is used to set password for TiDB\n# it must has Python MySQL client installed\nmysqlClient:\n  image: tnir/mysqlclient\n  imagePullPolicy: IfNotPresent\n\nmonitor:\n  create: true\n  # Also see rbac.create\n  # If you set rbac.create to false, you need to provide a value here.\n  # If you set rbac.create to true, you should leave this empty.\n  # serviceAccount:\n  persistent: false\n  storageClassName: local-storage\n  storage: 10Gi\n  grafana:\n    create: true\n    image: grafana/grafana:6.0.1\n    imagePullPolicy: IfNotPresent\n    logLevel: info\n    resources:\n      limits: {}\n      #   cpu: 8000m\n      #   memory: 8Gi\n      requests: {}\n      #   cpu: 4000m\n      #   memory: 4Gi\n    username: admin\n    password: admin\n    config:\n      # Configure Grafana using environment variables except GF_PATHS_DATA, GF_SECURITY_ADMIN_USER and GF_SECURITY_ADMIN_PASSWORD\n      # Ref https://grafana.com/docs/installation/configuration/#using-environment-variables\n      GF_AUTH_ANONYMOUS_ENABLED: \"true\"\n      GF_AUTH_ANONYMOUS_ORG_NAME: \"Main Org.\"\n      GF_AUTH_ANONYMOUS_ORG_ROLE: \"Viewer\"\n      # if grafana is running behind a reverse proxy with subpath http://foo.bar/grafana\n      # GF_SERVER_DOMAIN: foo.bar\n      # GF_SERVER_ROOT_URL: \"%(protocol)s://%(domain)s/grafana/\"\n    service:\n      type: NodePort\n  prometheus:\n    image: prom/prometheus:v2.2.1\n    imagePullPolicy: IfNotPresent\n    logLevel: info\n    resources:\n      limits: {}\n      #   cpu: 8000m\n      #   memory: 8Gi\n      requests: {}\n      #   cpu: 4000m\n      #   memory: 4Gi\n    service:\n      type: NodePort\n    reserveDays: 12\n    # alertmanagerURL: \"\"\n  nodeSelector: {}\n    # kind: monitor\n    # zone: cn-bj1-01,cn-bj1-02\n    # region: cn-bj1\n  tolerations: []\n  # - key: node-role\n  #   operator: Equal\n  #   value: tidb\n  #   effect: \"NoSchedule\"\n\nbinlog:\n  pump:\n    create: false\n    replicas: 1\n    image: pingcap/tidb-binlog:v3.0.0-rc.1\n    imagePullPolicy: IfNotPresent\n    logLevel: info\n    # storageClassName is a StorageClass provides a way for administrators to describe the \"classes\" of storage they offer.\n    # different classes might map to quality-of-service levels, or to backup policies,\n    # or to arbitrary policies determined by the cluster administrators.\n    # refer to https://kubernetes.io/docs/concepts/storage/storage-classes\n    storageClassName: local-storage\n    storage: 10Gi\n    syncLog: true\n    # a integer value to control expiry date of the binlog data, indicates for how long (in days) the binlog data would be stored.\n    # must bigger than 0\n    gc: 7\n    # number of seconds between heartbeat ticks (in 2 seconds)\n    heartbeatInterval: 2\n\n  drainer:\n    create: false\n    image: pingcap/tidb-binlog:v3.0.0-rc.1\n    imagePullPolicy: IfNotPresent\n    logLevel: info\n    # storageClassName is a StorageClass provides a way for administrators to describe the \"classes\" of storage they offer.\n    # different classes might map to quality-of-service levels, or to backup policies,\n    # or to arbitrary policies determined by the cluster administrators.\n    # refer to https://kubernetes.io/docs/concepts/storage/storage-classes\n    storageClassName: local-storage\n    storage: 10Gi\n    # the number of the concurrency of the downstream for synchronization. The bigger the value,\n    # the better throughput performance of the concurrency (16 by default)\n    workerCount: 16\n    # the interval time (in seconds) of detect pumps' status (default 10)\n    detectInterval: 10\n    # disbale detect causality\n    disableDetect: false\n    # disable dispatching sqls that in one same binlog; if set true, work-count and txn-batch would be useless\n    disableDispatch: false\n    # # disable sync these schema\n    ignoreSchemas: \"INFORMATION_SCHEMA,PERFORMANCE_SCHEMA,mysql,test\"\n    # if drainer donesn't have checkpoint, use initial commitTS to initial checkpoint\n    initialCommitTs: 0\n    # enable safe mode to make syncer reentrant\n    safeMode: false\n    # the number of SQL statements of a transaction that are output to the downstream database (20 by default)\n    txnBatch: 20\n    # downstream storage, equal to --dest-db-type\n    # valid values are \"mysql\", \"pb\", \"kafka\"\n    destDBType: pb\n    mysql: {}\n      # host: \"127.0.0.1\"\n      # user: \"root\"\n      # password: \"\"\n      # port: 3306\n      # # Time and size limits for flash batch write\n      # timeLimit: \"30s\"\n      # sizeLimit: \"100000\"\n    kafka: {}\n      # only need config one of zookeeper-addrs and kafka-addrs, will get kafka address if zookeeper-addrs is configed.\n      # zookeeperAddrs: \"127.0.0.1:2181\"\n      # kafkaAddrs: \"127.0.0.1:9092\"\n      # kafkaVersion: \"0.8.2.0\"\n\nscheduledBackup:\n  create: false\n  binlogImage: pingcap/tidb-binlog:v3.0.0-rc.1\n  binlogImagePullPolicy: IfNotPresent\n  # https://github.com/tennix/tidb-cloud-backup\n  mydumperImage: pingcap/tidb-cloud-backup:latest\n  mydumperImagePullPolicy: IfNotPresent\n  # storageClassName is a StorageClass provides a way for administrators to describe the \"classes\" of storage they offer.\n  # different classes might map to quality-of-service levels, or to backup policies,\n  # or to arbitrary policies determined by the cluster administrators.\n  # refer to https://kubernetes.io/docs/concepts/storage/storage-classes\n  storageClassName: local-storage\n  storage: 100Gi\n  # https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#schedule\n  schedule: \"0 0 * * *\"\n  # https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#suspend\n  suspend: false\n  # https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#jobs-history-limits\n  successfulJobsHistoryLimit: 3\n  failedJobsHistoryLimit: 1\n  # https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#starting-deadline\n  startingDeadlineSeconds: 3600\n  # https://github.com/maxbube/mydumper/blob/master/docs/mydumper_usage.rst#options\n  options: \"--chunk-filesize=100\"\n  # secretName is the name of the secret which stores user and password used for backup\n  # Note: you must give the user enough privilege to do the backup\n  # you can create the secret by:\n  # kubectl create secret generic backup-secret --from-literal=user=root --from-literal=password=\u003cpassword\u003e\n  secretName: backup-secret\n  # backup to gcp\n  gcp: {}\n  # bucket: \"\"\n  # secretName is the name of the secret which stores the gcp service account credentials json file\n  # The service account must have read/write permission to the above bucket.\n  # Read the following document to create the service account and download the credentials file as credentials.json:\n  # https://cloud.google.com/docs/authentication/production#obtaining_and_providing_service_account_credentials_manually\n  # And then create the secret by: kubectl create secret generic gcp-backup-secret --from-file=./credentials.json\n  # secretName: gcp-backup-secret\n\n  # backup to ceph object storage\n  ceph: {}\n  # endpoint: \"\"\n  # bucket: \"\"\n  # secretName is the name of the secret which stores ceph object store access key and secret key\n  # You can create the secret by:\n  # kubectl create secret generic ceph-backup-secret --from-literal=access_key=\u003caccess-key\u003e --from-literal=secret_key=\u003csecret-key\u003e\n  # secretName: ceph-backup-secret\n\nmetaInstance: \"{{ $labels.instance }}\"\nmetaType: \"{{ $labels.type }}\"\nmetaValue: \"{{ $value }}\"\n",
    "upstream": "github.com/pingcap/tidb-operator/charts/tidb-cluster",
    "metadata": {
      "applicationType": "helm",
      "sequence": 0,
      "name": "tidb-cluster",
      "releaseNotes": "upgrade default tidb to v3.0.0-rc.1 (#520)",
      "version": "dev",
      "license": {
        "id": "",
        "assignee": "",
        "createdAt": "0001-01-01T00:00:00Z",
        "expiresAt": "0001-01-01T00:00:00Z",
        "type": ""
      }
    },
    "contentSHA": "23f1c7b44fb246053ce187baf660a83f1eb2073d4024afec28a90dd19e29ef8a",
    "lifecycle": {
      "stepsCompleted": {
        "intro": true,
        "kustomize": true,
        "kustomize-intro": true,
        "outro": true,
        "render": true,
        "values": true
      }
    }
  }
}